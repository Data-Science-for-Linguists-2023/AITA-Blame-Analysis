# Progress Report

## 02/13/2023

* I've created the repository and filled out the necessary files
* Begun looking into using the Reddit API to create my corpus

## 1st Progress Report - 02/24/2023

I've begun the process of utilizing [PRAW](https://praw.readthedocs.io/en/latest/index.html) and [PMAW](https://github.com/mattpodolak/pmaw) to scrape data from [r/AmITheAsshole](https://www.reddit.com/r/AmItheAsshole/). I've discovered that it's difficult to use PRAW, the Python wrapper for the built-in Reddit API, alone for my needs. This is because the built-in Reddit API does not allow you to query posts past a certain duration of time (I'm still unsure what that duration is), and only lets you scrape 1,000 posts at a time. Because I'm only using one subreddit for this project, that does not give a satisfactory number of posts to use.
Luckily, there's a third-party API called the Pushshift API, which has the Python wrapper PMAW, which allows you to scrape all archived posts for a given subreddit. However, I came across a new problem: apparently, the Pushshift API is undergoing a migration and does not have access to any data from before November 2022 ([source](https://github.com/mattpodolak/pmaw/issues/57)). Still, AITA is a popular subreddit, so there is still a large amount of data available, but going forward it might be important to consider how small of a sliver temporally into the trends of speaking patterns people are exhibiting.
Lastly, it seems that limiting the amount of posts you are scraping is currently broken. If you set your limit above 1,000 posts, PMAW will just scrape everything available. As such, I am yet to have a successful run of scraping all of my data at once, and am doing it in batches in a notebook I currently have set to be ignored. I have a notebook that has a sample of the process available at "code/data_collection_testing.ipynb"

As for sharing plans, all of the data I use for this project should hypothetically already be available for people to view, so I think it would be safe to have my corpus publicly accessible. However, I might consider omitting the usernames of the posters. Although many people use a throwaway/temporary account to post to this subreddit, I am unsure whether publicizing the usernames in this corpus could result in some harm.

## 2nd Progress Report - 03/24/2023

I spent most of the time between now and trhe previous update figuring out any methods to improve my use of PMAW to get the reults I wanted, establishing some more specific questions I could explore in my data, and learning about potential methods to answering those new questions.

I ultimately discovered that there's still no solution to the post limit problem with PMAW and that there's no way to filter the search through the API based on link flair. So, the best solution I could determine was to just cast the widest net I feasibly could given the tools available and then pare down to what I wanted. I also discovered in this process that a vast majority of posts that are ever posted to this subreddit are eventually deleted, And so I start off with a corpus of 100,000 and end with one of just barely 9,500. I think, given the limits of the current tools, that might be the best that I'm able to get. The final data collection notebook is at [data_collection.ipynb](https://github.com/Data-Science-for-Linguists-2023/AITA-Blame-Analysis/blob/main/code/data_collection.ipynb) and the final data set can be found at [data_collection.ipynb](https://github.com/Data-Science-for-Linguists-2023/AITA-Blame-Analysis/blob/main/data/aita_data.csv).

As for questions, my main two goals were to think of things that could represent people obscuring or justifying their story, and then to think of the means currently possible in my skillset to explore them. To start out, I thought it would be easy to implement a few things from Homework 2 to see if post length could be a meanignful indicator. Then, I was curious about who/what is being focalized in these stories. I thought that testing for the sentence subjects would be a good indicator of that. I found the [spacy](https://spacy.io/) library in this process. A lot of my time was spent trying to get the library to work at all within Jupyter Notebook and understand how it works. I also found most of my time spent waiting for spacy to complete its processes before I could check if it did what I wanted. I'm not sure whether doing these operations directly within the DataFrame is adding meaningful overhead or if it just takes a long time to use spacy in general, but it takes around half an hour to process through the whole DataFrame on my machine. The analysis I do is not the most for now, but I've set up the methods to which I can easily explore questions--like counting the use of the passive and the use of each pronoun--using spacy to analyze dependencies. Data analysis can be found at [data_analysis.ipynb](https://github.com/Data-Science-for-Linguists-2023/AITA-Blame-Analysis/blob/main/code/data_analysis.ipynb).